{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Level 1 (Basic) Task 1: Data Collection and Web Scraping:"
      ],
      "metadata": {
        "id": "8Wt-u8gP_Iw3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miGqIDm78_Sf",
        "outputId": "0e12354b-587e-4064-9558-326da64315d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting our web scraping robot...\n",
            "\n",
            "--- Step 1: Fetching a Single Page ---\n",
            "Attempting to fetch URL: http://books.toscrape.com/\n",
            "Successfully fetched the page content.\n",
            "Page content parsed successfully with BeautifulSoup.\n",
            "\n",
            "--- Step 2: Extracting Data from the Page ---\n",
            "Found 20 book articles on this page.\n",
            "Extracted data from the first page:\n",
            "  - Title: A Light in the Attic, Price: Â£51.77, Rating: 3 out of 5\n",
            "  - Title: Tipping the Velvet, Price: Â£53.74, Rating: 1 out of 5\n",
            "  - Title: Soumission, Price: Â£50.10, Rating: 1 out of 5\n",
            "  - Title: Sharp Objects, Price: Â£47.82, Rating: 4 out of 5\n",
            "  - Title: Sapiens: A Brief History of Humankind, Price: Â£54.23, Rating: 5 out of 5\n",
            "\n",
            "--- Step 3: Handling Pagination ---\n",
            "Scraping page 1: http://books.toscrape.com/\n",
            "Scraping page 2: http://books.toscrape.com/catalogue/page-2.html\n",
            "Scraping page 3: http://books.toscrape.com/catalogue/page-3.html\n",
            "Scraping page 4: http://books.toscrape.com/catalogue/page-4.html\n",
            "Scraping page 5: http://books.toscrape.com/catalogue/page-5.html\n",
            "Scraping page 6: http://books.toscrape.com/catalogue/page-6.html\n",
            "Scraping page 7: http://books.toscrape.com/catalogue/page-7.html\n",
            "Scraping page 8: http://books.toscrape.com/catalogue/page-8.html\n",
            "Scraping page 9: http://books.toscrape.com/catalogue/page-9.html\n",
            "Scraping page 10: http://books.toscrape.com/catalogue/page-10.html\n",
            "Scraping page 11: http://books.toscrape.com/catalogue/page-11.html\n",
            "Scraping page 12: http://books.toscrape.com/catalogue/page-12.html\n",
            "Scraping page 13: http://books.toscrape.com/catalogue/page-13.html\n",
            "Scraping page 14: http://books.toscrape.com/catalogue/page-14.html\n",
            "Scraping page 15: http://books.toscrape.com/catalogue/page-15.html\n",
            "Scraping page 16: http://books.toscrape.com/catalogue/page-16.html\n",
            "Scraping page 17: http://books.toscrape.com/catalogue/page-17.html\n",
            "Scraping page 18: http://books.toscrape.com/catalogue/page-18.html\n",
            "Scraping page 19: http://books.toscrape.com/catalogue/page-19.html\n",
            "Scraping page 20: http://books.toscrape.com/catalogue/page-20.html\n",
            "Scraping page 21: http://books.toscrape.com/catalogue/page-21.html\n",
            "Scraping page 22: http://books.toscrape.com/catalogue/page-22.html\n",
            "Scraping page 23: http://books.toscrape.com/catalogue/page-23.html\n",
            "Scraping page 24: http://books.toscrape.com/catalogue/page-24.html\n",
            "Scraping page 25: http://books.toscrape.com/catalogue/page-25.html\n",
            "Scraping page 26: http://books.toscrape.com/catalogue/page-26.html\n",
            "Scraping page 27: http://books.toscrape.com/catalogue/page-27.html\n",
            "Scraping page 28: http://books.toscrape.com/catalogue/page-28.html\n",
            "Scraping page 29: http://books.toscrape.com/catalogue/page-29.html\n",
            "Scraping page 30: http://books.toscrape.com/catalogue/page-30.html\n",
            "Scraping page 31: http://books.toscrape.com/catalogue/page-31.html\n",
            "Scraping page 32: http://books.toscrape.com/catalogue/page-32.html\n",
            "Scraping page 33: http://books.toscrape.com/catalogue/page-33.html\n",
            "Scraping page 34: http://books.toscrape.com/catalogue/page-34.html\n",
            "Scraping page 35: http://books.toscrape.com/catalogue/page-35.html\n",
            "Scraping page 36: http://books.toscrape.com/catalogue/page-36.html\n",
            "Scraping page 37: http://books.toscrape.com/catalogue/page-37.html\n",
            "Scraping page 38: http://books.toscrape.com/catalogue/page-38.html\n",
            "Scraping page 39: http://books.toscrape.com/catalogue/page-39.html\n",
            "Scraping page 40: http://books.toscrape.com/catalogue/page-40.html\n",
            "Scraping page 41: http://books.toscrape.com/catalogue/page-41.html\n",
            "Scraping page 42: http://books.toscrape.com/catalogue/page-42.html\n",
            "Scraping page 43: http://books.toscrape.com/catalogue/page-43.html\n",
            "Scraping page 44: http://books.toscrape.com/catalogue/page-44.html\n",
            "Scraping page 45: http://books.toscrape.com/catalogue/page-45.html\n",
            "Scraping page 46: http://books.toscrape.com/catalogue/page-46.html\n",
            "Scraping page 47: http://books.toscrape.com/catalogue/page-47.html\n",
            "Scraping page 48: http://books.toscrape.com/catalogue/page-48.html\n",
            "Scraping page 49: http://books.toscrape.com/catalogue/page-49.html\n",
            "Scraping page 50: http://books.toscrape.com/catalogue/page-50.html\n",
            "No 'Next' button found. All pages scraped.\n",
            "\n",
            "Total books scraped across all pages: 1000\n",
            "\n",
            "--- Step 4: Storing Data in a Structured Format (CSV) ---\n",
            "Data successfully saved to 'scraped_books_data.csv'.\n",
            "You can open this file with Excel or any spreadsheet program.\n",
            "\n",
            "Web scraping robot finished its mission!\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "print(\"Starting our web scraping robot...\")\n",
        "\n",
        "# --- Part 1: Fetching a Single Page ---\n",
        "print(\"\\n--- Step 1: Fetching a Single Page ---\")\n",
        "url = 'http://books.toscrape.com/'\n",
        "print(f\"Attempting to fetch URL: {url}\")\n",
        "\n",
        "try:\n",
        "    # Send a request to the website to get its content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # This will raise an HTTPError for bad responses (4xx or 5xx)\n",
        "    print(\"Successfully fetched the page content.\")\n",
        "\n",
        "    # Parse the HTML content using BeautifulSoup\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "    print(\"Page content parsed successfully with BeautifulSoup.\")\n",
        "\n",
        "    # --- Part 2: Extracting Data from the Page ---\n",
        "    print(\"\\n--- Step 2: Extracting Data from the Page ---\")\n",
        "    books_data = [] # A list to store all the book information\n",
        "\n",
        "    # Find all book articles on the page\n",
        "    # Each book is enclosed in an <article> tag with class 'product_pod'\n",
        "    book_articles = soup.find_all('article', class_='product_pod')\n",
        "    print(f\"Found {len(book_articles)} book articles on this page.\")\n",
        "\n",
        "    for book in book_articles:\n",
        "        # Extract Title\n",
        "        # Title is in an <h3> tag, inside an <a> tag's 'title' attribute\n",
        "        title_tag = book.find('h3').find('a')\n",
        "        title = title_tag['title'] if title_tag else 'N/A'\n",
        "\n",
        "        # Extract Price\n",
        "        # Price is in a <p> tag with class 'price_color'\n",
        "        price_tag = book.find('p', class_='price_color')\n",
        "        price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
        "\n",
        "        # Extract Rating\n",
        "        # Rating is in a <p> tag with class 'star-rating' followed by the rating word\n",
        "        rating_tag = book.find('p', class_=lambda x: x and 'star-rating' in x.split())\n",
        "        rating = 'N/A'\n",
        "        if rating_tag:\n",
        "            # The rating word is the second class (e.g., 'Three' in 'star-rating Three')\n",
        "            # We convert it to a more readable format\n",
        "            rating_classes = rating_tag['class']\n",
        "            if len(rating_classes) > 1:\n",
        "                rating_word = rating_classes[1]\n",
        "                rating_map = {\n",
        "                    'One': '1 out of 5',\n",
        "                    'Two': '2 out of 5',\n",
        "                    'Three': '3 out of 5',\n",
        "                    'Four': '4 out of 5',\n",
        "                    'Five': '5 out of 5'\n",
        "                }\n",
        "                rating = rating_map.get(rating_word, 'Unknown Rating')\n",
        "\n",
        "        # Add the extracted data to our list\n",
        "        books_data.append({\n",
        "            'Title': title,\n",
        "            'Price': price,\n",
        "            'Rating': rating\n",
        "        })\n",
        "\n",
        "    print(\"Extracted data from the first page:\")\n",
        "    for book in books_data[:5]: # Print first 5 for a quick check\n",
        "        print(f\"  - Title: {book['Title']}, Price: {book['Price']}, Rating: {book['Rating']}\")\n",
        "\n",
        "    # --- Part 3: Handling Pagination (Going through multiple pages) ---\n",
        "    print(\"\\n--- Step 3: Handling Pagination ---\")\n",
        "    all_books_data = [] # A new list to store data from all pages\n",
        "    base_url = 'http://books.toscrape.com/catalogue/' # Base URL for pagination\n",
        "    page_num = 1\n",
        "    has_next_page = True\n",
        "\n",
        "    while has_next_page:\n",
        "        current_page_url = f'{base_url}page-{page_num}.html' if page_num > 1 else url\n",
        "        print(f\"Scraping page {page_num}: {current_page_url}\")\n",
        "\n",
        "        try:\n",
        "            page_response = requests.get(current_page_url)\n",
        "            page_response.raise_for_status()\n",
        "            page_soup = BeautifulSoup(page_response.text, 'html.parser')\n",
        "\n",
        "            # Extract data from the current page (same logic as before)\n",
        "            current_page_books = page_soup.find_all('article', class_='product_pod')\n",
        "            if not current_page_books: # If no books found, assume end of pages\n",
        "                print(f\"No more books found on page {page_num}. Ending pagination.\")\n",
        "                has_next_page = False\n",
        "                continue # Skip to the next iteration to exit loop\n",
        "\n",
        "            for book in current_page_books:\n",
        "                title_tag = book.find('h3').find('a')\n",
        "                title = title_tag['title'] if title_tag else 'N/A'\n",
        "\n",
        "                price_tag = book.find('p', class_='price_color')\n",
        "                price = price_tag.get_text(strip=True) if price_tag else 'N/A'\n",
        "\n",
        "                rating_tag = book.find('p', class_=lambda x: x and 'star-rating' in x.split())\n",
        "                rating = 'N/A'\n",
        "                if rating_tag:\n",
        "                    rating_classes = rating_tag['class']\n",
        "                    if len(rating_classes) > 1:\n",
        "                        rating_word = rating_classes[1]\n",
        "                        rating_map = {\n",
        "                            'One': '1 out of 5', 'Two': '2 out of 5', 'Three': '3 out of 5',\n",
        "                            'Four': '4 out of 5', 'Five': '5 out of 5'\n",
        "                        }\n",
        "                        rating = rating_map.get(rating_word, 'Unknown Rating')\n",
        "\n",
        "                all_books_data.append({\n",
        "                    'Title': title,\n",
        "                    'Price': price,\n",
        "                    'Rating': rating\n",
        "                })\n",
        "\n",
        "            # Check for \"next\" button to determine if there's another page\n",
        "            next_button = page_soup.find('li', class_='next')\n",
        "            if next_button:\n",
        "                page_num += 1\n",
        "            else:\n",
        "                has_next_page = False\n",
        "                print(\"No 'Next' button found. All pages scraped.\")\n",
        "\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"Error fetching page {current_page_url}: {e}\")\n",
        "            has_next_page = False # Stop if there's an error\n",
        "\n",
        "    print(f\"\\nTotal books scraped across all pages: {len(all_books_data)}\")\n",
        "\n",
        "    # --- Part 4: Storing Data in a Structured Format (CSV) ---\n",
        "    print(\"\\n--- Step 4: Storing Data in a Structured Format (CSV) ---\")\n",
        "    # Convert the list of dictionaries into a pandas DataFrame\n",
        "    df = pd.DataFrame(all_books_data)\n",
        "\n",
        "    # Save the DataFrame to a CSV file\n",
        "    csv_filename = 'scraped_books_data.csv'\n",
        "    df.to_csv(csv_filename, index=False, encoding='utf-8')\n",
        "    print(f\"Data successfully saved to '{csv_filename}'.\")\n",
        "    print(\"You can open this file with Excel or any spreadsheet program.\")\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"An error occurred during the initial request: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "print(\"\\nWeb scraping robot finished its mission!\")"
      ]
    }
  ]
}